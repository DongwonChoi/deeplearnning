{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 7-1 ML의 실용, 팁\n",
    "####  leaning rate 정하는 법- 특별한 답은 없고 0.01로 시작해서 값을 조정하면 적당한\n",
    "#### 값을 탐색\n",
    "#### nomalize data, zero-centered data 등으로 preprocessing이 필요\n",
    "#### x'=x-M/분산\n",
    "#### overfitting - 학습데이터에 딱맞는 모델을 만들면 실제 사용시 잘 맞지 않는 현상?\n",
    "#### 데이터가 많으면 많을 수록 overfitting을 줄임, regularzation사용해서 줄임\n",
    "#### 모델 모양을 구부리지 말고 펴지게 함 weight를 줄임(?)\n",
    "#### tensorplow 구현시 l2reg라는 변수 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 7-1\n",
    "#### 모델이 얼마나 훌룡한가? 를 평가하는 법\n",
    "#### -> training set과 test set을 구분해서 사용하고 test set은 학습시 사용 x\n",
    "#### test set으로 실제로 얼마나 잘 맞나 평가\n",
    "#### 알파와 람다값을 조정할 필요가 있으면 training set을 training과 validation으로\n",
    "#### 나누게됨\n",
    "#### online learning - 한번에 너무 많은 데이터를 학습시키려 할때 나눠서 학습시킴\n",
    "#### 95~99% 정확도면 좋은 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.1290576 [[-1.2895828   0.9119962   1.651022  ]\n",
      " [ 0.4556617   0.08549981 -0.29683048]\n",
      " [ 1.1107783   1.3890737  -0.06849846]]\n",
      "1 1.8079677 [[-1.3131317   0.9008589   1.6857082 ]\n",
      " [ 0.385736    0.06151982 -0.20292476]\n",
      " [ 1.0551319   1.3606158   0.0156058 ]]\n",
      "2 1.5387187 [[-1.3278095   0.88253117  1.7187136 ]\n",
      " [ 0.35994962 -0.00266764 -0.11295092]\n",
      " [ 1.0415611   1.2925256   0.09726681]]\n",
      "3 1.2931585 [[-1.3508314   0.875025    1.7492418 ]\n",
      " [ 0.2870541  -0.01355356 -0.02916946]\n",
      " [ 0.9804964   1.276221    0.17463623]]\n",
      "4 1.0826633 [[-1.3609421   0.8580373   1.7763402 ]\n",
      " [ 0.27863556 -0.07935654  0.04505207]\n",
      " [ 0.9812247   1.2054399   0.24468902]]\n",
      "5 0.92299914 [[-1.3815151   0.8563379   1.7986127 ]\n",
      " [ 0.20991552 -0.06935525  0.10377079]\n",
      " [ 0.92106986  1.2092259   0.3010579 ]]\n",
      "6 0.8290444 [[-1.3854935   0.8438701   1.8150588 ]\n",
      " [ 0.22736195 -0.12244396  0.13941309]\n",
      " [ 0.9441987   1.1505294   0.33662555]]\n",
      "7 0.79589033 [[-1.4003322   0.8480091   1.8257586 ]\n",
      " [ 0.18352453 -0.09008335  0.1508899 ]\n",
      " [ 0.9055723   1.1773264   0.34845495]]\n",
      "8 0.7870842 [[-1.4016593   0.8409268   1.8341678 ]\n",
      " [ 0.21294242 -0.11931416  0.15070286]\n",
      " [ 0.93870354  1.1424325   0.35021773]]\n",
      "9 0.7813905 [[-1.4131932   0.84484076  1.8417879 ]\n",
      " [ 0.1860705  -0.08993175  0.14819238]\n",
      " [ 0.91603047  1.1665742   0.34874895]]\n",
      "10 0.776682 [[-1.415852    0.8397239   1.8495636 ]\n",
      " [ 0.20779951 -0.10916775  0.1456994 ]\n",
      " [ 0.9413176   1.1419424   0.34809363]]\n",
      "11 0.7727136 [[-1.4259506   0.84230393  1.8570822 ]\n",
      " [ 0.18847786 -0.08724465  0.14309794]\n",
      " [ 0.9259387   1.1588131   0.34660175]]\n",
      "12 0.7692522 [[-1.4296583   0.8383453   1.8647486 ]\n",
      " [ 0.20417845 -0.10060307  0.14075577]\n",
      " [ 0.94509697  1.1403491   0.3459075 ]]\n",
      "13 0.7661425 [[-1.4387871   0.8400224   1.8722004 ]\n",
      " [ 0.18990192 -0.08385251  0.13828176]\n",
      " [ 0.9345287   1.152272    0.3445529 ]]\n",
      "14 0.7632822 [[-1.4431984   0.83687735  1.8797567 ]\n",
      " [ 0.20147441 -0.09319101  0.13604778]\n",
      " [ 0.94942933  1.1381104   0.34381384]]\n",
      "15 0.76060915 [[-1.4516279   0.83793354  1.88713   ]\n",
      " [ 0.19077371 -0.08011421  0.13367169]\n",
      " [ 0.94221884  1.1465948   0.34253997]]\n",
      "16 0.75807536 [[-1.4565225   0.83538157  1.8945765 ]\n",
      " [ 0.19942985 -0.08661316  0.1315145 ]\n",
      " [ 0.9540629   1.1355417   0.341749  ]]\n",
      "17 0.75565326 [[-1.4644315   0.836001    1.9018661 ]\n",
      " [ 0.19132937 -0.07622035  0.12922218]\n",
      " [ 0.9492527   1.1415757   0.34052515]]\n",
      "18 0.7533188 [[-1.4696616   0.8338918   1.9092053 ]\n",
      " [ 0.1978821  -0.08067739  0.12712651]\n",
      " [ 0.9588518   1.132818    0.33968377]]\n",
      "19 0.7510576 [[-1.4771739   0.8342004   1.916409  ]\n",
      " [ 0.19170567 -0.07228454  0.12491009]\n",
      " [ 0.95578337  1.1370785   0.33849174]]\n",
      "20 0.74885726 [[-1.4826368   0.83242804  1.9236442 ]\n",
      " [ 0.19671991 -0.07525682  0.12286814]\n",
      " [ 0.96370596  1.1300434   0.33760428]]\n",
      "21 0.74670976 [[-1.4898404   0.83251435  1.9307615 ]\n",
      " [ 0.19198634 -0.06837735  0.12072227]\n",
      " [ 0.9619142   1.1330072   0.33643234]]\n",
      "22 0.74460804 [[-1.495463    0.8310025   1.937896  ]\n",
      " [ 0.19586381 -0.07026237  0.11872983]\n",
      " [ 0.96856797  1.1272819   0.33550376]]\n",
      "23 0.7425475 [[-1.5024215   0.83093     1.9449271 ]\n",
      " [ 0.1922241  -0.06454318  0.11665036]\n",
      " [ 0.96771854  1.1292912   0.3343439 ]]\n",
      "24 0.7405244 [[-1.5081513   0.82962257  1.9519643 ]\n",
      " [ 0.19525506 -0.06562882  0.11470503]\n",
      " [ 0.973401    1.1245737   0.33337894]]\n",
      "25 0.7385355 [[-1.5149117   0.8294373   1.9589099 ]\n",
      " [ 0.19245268 -0.06080981  0.11268839]\n",
      " [ 0.9732507   1.1258774   0.33222553]]\n",
      "26 0.7365782 [[-1.5207101   0.8282927   1.9658529 ]\n",
      " [ 0.1948492  -0.06130666  0.11078874]\n",
      " [ 0.97818166  1.1219435   0.3312285 ]]\n",
      "27 0.73465097 [[-1.5273072   0.82802856  1.9727141 ]\n",
      " [ 0.19269325 -0.05719377  0.1088318 ]\n",
      " [ 0.97855157  1.1227245   0.3300775 ]]\n",
      "28 0.7327518 [[-1.533146    0.8270156   1.979566  ]\n",
      " [ 0.1946119  -0.05725747  0.10697684]\n",
      " [ 0.98289543  1.119406    0.32905215]]\n",
      "29 0.73087966 [[-1.539606    0.82669747  1.986344  ]\n",
      " [ 0.19295892 -0.05370434  0.10507671]\n",
      " [ 0.983653    1.1198      0.32790056]]\n",
      "30 0.72903275 [[-1.5454643   0.8257924   1.9931074 ]\n",
      " [ 0.1945161  -0.05345043  0.10326561]\n",
      " [ 0.9875339   1.1169695   0.32685012]]\n",
      "31 0.72721094 [[-1.551807    0.8254388   1.9998038 ]\n",
      " [ 0.19325736 -0.05034579  0.10141973]\n",
      " [ 0.98858035  1.1170772   0.3256959 ]]\n",
      "32 0.72541267 [[-1.5576694   0.8246234   2.0064814 ]\n",
      " [ 0.19454008 -0.0498605   0.09965172]\n",
      " [ 0.9920928   1.1146375   0.3246231 ]]\n",
      "33 0.7236374 [[-1.5639105   0.82424825  2.0130975 ]\n",
      " [ 0.19359255 -0.0471189   0.09785767]\n",
      " [ 0.9933541   1.1145346   0.3234647 ]]\n",
      "34 0.72188437 [[-1.5697651   0.8235085   2.019692  ]\n",
      " [ 0.19466625 -0.04646692  0.09613197]\n",
      " [ 0.99657065  1.1124108   0.3223719 ]]\n",
      "35 0.72015303 [[-1.5759163   0.8231221   2.0262296 ]\n",
      " [ 0.19396588 -0.04402213  0.09438756]\n",
      " [ 0.9979911   1.112154    0.32120824]]\n",
      "36 0.71844244 [[-1.5817547   0.82244706  2.032743  ]\n",
      " [ 0.19487993 -0.04325203  0.0927034 ]\n",
      " [ 1.0009679   1.1102879   0.32009748]]\n",
      "37 0.7167525 [[-1.5878253   0.82205707  2.0392036 ]\n",
      " [ 0.19437715 -0.04105242  0.09100658]\n",
      " [ 1.0025053   1.1099201   0.31892782]]\n",
      "38 0.7150824 [[-1.5936409   0.8214381   2.0456383 ]\n",
      " [ 0.19516905 -0.04020089  0.08936315]\n",
      " [ 1.0052862   1.1082662   0.31780088]]\n",
      "39 0.7134315 [[-1.5996383   0.8210503   2.0520234 ]\n",
      " [ 0.19482504 -0.03820577  0.08771203]\n",
      " [ 1.0069089   1.1078197   0.3166248 ]]\n",
      "40 0.71179986 [[-1.6054265   0.8204805   2.0583813 ]\n",
      " [ 0.19552319 -0.03730037  0.0861085 ]\n",
      " [ 1.0095283   1.106342    0.31548318]]\n",
      "41 0.7101866 [[-1.6113566   0.8200993   2.0646927 ]\n",
      " [ 0.19530743 -0.03547742  0.08450131]\n",
      " [ 1.011212    1.105841    0.31430042]]\n",
      "42 0.7085916 [[-1.6171138   0.8195733   2.070976  ]\n",
      " [ 0.19593354 -0.03453894  0.08293673]\n",
      " [ 1.0136971   1.1045109   0.31314543]]\n",
      "43 0.70701414 [[-1.6229814   0.81920177  2.0772152 ]\n",
      " [ 0.19582182 -0.0328623   0.08137181]\n",
      " [ 1.0154234   1.1039742   0.31195587]]\n",
      "44 0.7054539 [[-1.628705    0.818715    2.0834255 ]\n",
      " [ 0.19639254 -0.03190644  0.07984525]\n",
      " [ 1.017796    1.1027688   0.3107887 ]]\n",
      "45 0.70391077 [[-1.6345141   0.8183555   2.0895941 ]\n",
      " [ 0.1963656  -0.03035531  0.07832106]\n",
      " [ 1.019551    1.10221     0.3095924 ]]\n",
      "46 0.70238435 [[-1.6402023   0.81790435  2.0957334 ]\n",
      " [ 0.19689347 -0.02939372  0.07683161]\n",
      " [ 1.0218287   1.1011108   0.30841407]]\n",
      "47 0.70087385 [[-1.645956    0.81755865  2.1018329 ]\n",
      " [ 0.1969358  -0.02795114  0.07534669]\n",
      " [ 1.0236018   1.1005406   0.3072111 ]]\n",
      "48 0.6993794 [[-1.6516075   0.8171401   2.107903  ]\n",
      " [ 0.1974305  -0.02699257  0.07389341]\n",
      " [ 1.0257984   1.0995325   0.30602258]]\n",
      "49 0.6979004 [[-1.6573087   0.8168094   2.1139348 ]\n",
      " [ 0.19752957 -0.02564462  0.0724464 ]\n",
      " [ 1.0275817   1.0989587   0.30481318]]\n",
      "50 0.6964368 [[-1.6629227   0.8164209   2.1199372 ]\n",
      " [ 0.19799852 -0.02469553  0.07102835]\n",
      " [ 1.029709    1.0980293   0.30361536]]\n",
      "51 0.6949883 [[-1.6685736   0.816106    2.125903  ]\n",
      " [ 0.19814418 -0.02343074  0.0696179 ]\n",
      " [ 1.0314963   1.0974575   0.3023997 ]]\n",
      "52 0.6935544 [[-1.6741496   0.8157455   2.1318395 ]\n",
      " [ 0.198593   -0.02249576  0.0682341 ]\n",
      " [ 1.0335636   1.0965966   0.30119327]]\n",
      "53 0.69213486 [[-1.6797522   0.815447    2.1377406 ]\n",
      " [ 0.19877687 -0.02130451  0.06685898]\n",
      " [ 1.0353506   1.0960313   0.2999716 ]]\n",
      "54 0.6907296 [[-1.68529     0.8151126   2.1436126 ]\n",
      " [ 0.19920981 -0.02038702  0.06550856]\n",
      " [ 1.0373657   1.0952305   0.29875734]]\n",
      "55 0.689338 [[-1.6908461   0.8148308   2.1494505 ]\n",
      " [ 0.19942513 -0.01926142  0.06416764]\n",
      " [ 1.0391489   1.0946746   0.29753003]]\n",
      "56 0.68796 [[-1.6963454   0.81452096  2.1552596 ]\n",
      " [ 0.1998454  -0.01836366  0.06284963]\n",
      " [ 1.0411184   1.0939265   0.29630855]]\n",
      "57 0.6865957 [[-1.7018566   0.81425595  2.1610358 ]\n",
      " [ 0.20008643 -0.01729677  0.06154174]\n",
      " [ 1.0428954   1.0933822   0.29507586]]\n",
      "58 0.68524414 [[-1.7073177   0.8139694   2.1667836 ]\n",
      " [ 0.2004965  -0.01642034  0.06025524]\n",
      " [ 1.0448247   1.092681    0.29384777]]\n",
      "59 0.6839056 [[-1.7127854   0.81372124  2.1724994 ]\n",
      " [ 0.20075852 -0.01540643  0.05897931]\n",
      " [ 1.0465938   1.0921497   0.29261002]]\n",
      "60 0.6825797 [[-1.7182084   0.8134567   2.1781871 ]\n",
      " [ 0.20116019 -0.01455221  0.0577234 ]\n",
      " [ 1.0484874   1.0914903   0.29137588]]\n",
      "61 0.6812663 [[-1.7236338   0.8132254   2.1838439 ]\n",
      " [ 0.2014393  -0.01358633  0.0564784 ]\n",
      " [ 1.0502473   1.0909729   0.29013342]]\n",
      "62 0.679965 [[-1.7290192   0.81298184  2.1894727 ]\n",
      " [ 0.20183392 -0.0127548   0.05525228]\n",
      " [ 1.0521091   1.0903507   0.28889376]]\n",
      "63 0.6786759 [[-1.7344034   0.8127672   2.1950715 ]\n",
      " [ 0.20212682 -0.01183267  0.05403725]\n",
      " [ 1.053859    1.0898477   0.28764692]]\n",
      "64 0.6773983 [[-1.7397515   0.8125437   2.200643  ]\n",
      " [ 0.20251533 -0.01102392  0.05283998]\n",
      " [ 1.0556924   1.089259    0.2864022 ]]\n",
      "65 0.6761325 [[-1.7450956   0.8123455   2.2061853 ]\n",
      " [ 0.20281915 -0.01014172  0.05165394]\n",
      " [ 1.0574318   1.0887705   0.28515127]]\n",
      "66 0.67487806 [[-1.7504067   0.81214124  2.2117007 ]\n",
      " [ 0.20320223 -0.00935561  0.05048475]\n",
      " [ 1.0592396   1.088212    0.283902  ]]\n",
      "67 0.6736346 [[-1.7557118   0.81195927  2.2171876 ]\n",
      " [ 0.20351462 -0.00851006  0.04932684]\n",
      " [ 1.0609682   1.0877382   0.28264734]]\n",
      "68 0.6724025 [[-1.7609864   0.8117734   2.2226481 ]\n",
      " [ 0.20389293 -0.00774643  0.04818489]\n",
      " [ 1.0627528   1.0872068   0.28139392]]\n",
      "69 0.67118096 [[-1.7662532   0.81160736  2.228081  ]\n",
      " [ 0.20421183 -0.00693461  0.04705419]\n",
      " [ 1.0644705   1.0867473   0.2801358 ]]\n",
      "70 0.66997015 [[-1.771492    0.8114392   2.2334878 ]\n",
      " [ 0.20458548 -0.00619277  0.04593872]\n",
      " [ 1.0662342   1.0862408   0.27887866]]\n",
      "71 0.6687697 [[-1.7767215   0.81128883  2.2388678 ]\n",
      " [ 0.20490916 -0.00541215  0.04483441]\n",
      " [ 1.067941    1.0857953   0.27761742]]\n",
      "72 0.66757965 [[-1.7819248   0.8111378   2.2442222 ]\n",
      " [ 0.20527852 -0.00469174  0.04374463]\n",
      " [ 1.0696856   1.0853112   0.2763569 ]]\n",
      "73 0.6663998 [[-1.7871178   0.81100273  2.24955   ]\n",
      " [ 0.20560554 -0.00394003  0.04266591]\n",
      " [ 1.0713816   1.0848792   0.27509284]]\n",
      "74 0.6652299 [[-1.7922864   0.8108682   2.2548532 ]\n",
      " [ 0.20597054 -0.00324022  0.04160109]\n",
      " [ 1.0731086   1.0844157   0.2738293 ]]\n",
      "75 0.66406983 [[-1.7974436   0.8107481   2.2601306 ]\n",
      " [ 0.20629975 -0.00251552  0.04054717]\n",
      " [ 1.0747942   1.0839967   0.2725627 ]]\n",
      "76 0.6629193 [[-1.8025779e+00  8.1062955e-01  2.2653835e+00]\n",
      " [ 2.0666043e-01 -1.8356033e-03  3.9506570e-02]\n",
      " [ 1.0765051e+00  1.0835520e+00  2.7129650e-01]]\n",
      "77 0.66177833 [[-1.8077000e+00  8.1052411e-01  2.2706110e+00]\n",
      " [ 2.0699067e-01 -1.1360026e-03  3.8476728e-02]\n",
      " [ 1.0781806e+00  1.0831455e+00  2.7002764e-01]]\n",
      "78 0.66064656 [[-1.8128005e+00  8.1042099e-01  2.2758145e+00]\n",
      " [ 2.0734698e-01 -4.7525531e-04  3.7459686e-02]\n",
      " [ 1.0798765e+00  1.0827181e+00  2.6875910e-01]]\n",
      "79 0.65952426 [[-1.8178885e+00  8.1032991e-01  2.2809935e+00]\n",
      " [ 2.0767736e-01  2.0079844e-04  3.6453232e-02]\n",
      " [ 1.0815421e+00  1.0823233e+00  2.6748830e-01]]\n",
      "80 0.6584109 [[-1.8229557e+00  8.1024176e-01  2.2861488e+00]\n",
      " [ 2.0802921e-01  8.4316666e-04  3.5459030e-02]\n",
      " [ 1.0832243e+00  1.0819118e+00  2.6621771e-01]]\n",
      "81 0.65730643 [[-1.8280101e+00  8.1016469e-01  2.2912803e+00]\n",
      " [ 2.0835896e-01  1.4972293e-03  3.4475226e-02]\n",
      " [ 1.0848804e+00  1.0815283e+00  2.6494521e-01]]\n",
      "82 0.65621084 [[-1.8330446e+00  8.1009102e-01  2.2963886e+00]\n",
      " [ 2.0870630e-01  2.1218951e-03  3.3503208e-02]\n",
      " [ 1.0865496e+00  1.0811313e+00  2.6367286e-01]]\n",
      "83 0.65512383 [[-1.8380661   0.81002766  2.3014734 ]\n",
      " [ 0.20903471  0.00275531  0.03254138]\n",
      " [ 1.0881964   1.0807586   0.26239887]]\n",
      "84 0.65404534 [[-1.8430686   0.80996805  2.3065355 ]\n",
      " [ 0.2093776   0.0033629   0.03159091]\n",
      " [ 1.0898539   1.080375    0.26112503]]\n",
      "85 0.6529753 [[-1.8480577   0.80991805  2.3115747 ]\n",
      " [ 0.209704    0.00397699  0.03065041]\n",
      " [ 1.0914917   1.0800123   0.2598498 ]]\n",
      "86 0.6519136 [[-1.8530287   0.80987215  2.3165915 ]\n",
      " [ 0.21004227  0.00456825  0.02972089]\n",
      " [ 1.0931381   1.079641    0.2585747 ]]\n",
      "87 0.6508601 [[-1.8579861   0.8098352   2.321586  ]\n",
      " [ 0.21036613  0.00516415  0.02880113]\n",
      " [ 1.0947673   1.079288    0.25729847]]\n",
      "88 0.64981455 [[-1.862926    0.8098026   2.3265584 ]\n",
      " [ 0.21069963  0.00573981  0.02789197]\n",
      " [ 1.0964034   1.0789281   0.25602242]]\n",
      "89 0.6487768 [[-1.8678524   0.8097784   2.3315089 ]\n",
      " [ 0.2110206   0.00631848  0.02699234]\n",
      " [ 1.0980244   1.0785841   0.2547454 ]]\n",
      "90 0.64774686 [[-1.8727618   0.80975866  2.336438  ]\n",
      " [ 0.21134925  0.00687921  0.02610296]\n",
      " [ 1.0996505   1.0782347   0.25346857]]\n",
      "91 0.6467247 [[-1.8776578   0.8097469   2.3413455 ]\n",
      " [ 0.21166694  0.00744159  0.02522288]\n",
      " [ 1.1012636   1.0778991   0.252191  ]]\n",
      "92 0.6457101 [[-1.8825372   0.80973977  2.3462322 ]\n",
      " [ 0.21199065  0.00798807  0.02435268]\n",
      " [ 1.1028806   1.0775595   0.25091362]]\n",
      "93 0.64470285 [[-1.8874032   0.8097402   2.3510978 ]\n",
      " [ 0.2123047   0.00853512  0.02349159]\n",
      " [ 1.1044861   1.0772319   0.24963567]]\n",
      "94 0.6437032 [[-1.8922533   0.8097453   2.3559427 ]\n",
      " [ 0.21262342  0.00906795  0.02264006]\n",
      " [ 1.1060945   1.0769013   0.24835795]]\n",
      "95 0.64271057 [[-1.8970898   0.80975753  2.3607671 ]\n",
      " [ 0.21293351  0.00960047  0.02179743]\n",
      " [ 1.1076926   1.0765812   0.24707983]]\n",
      "96 0.6417254 [[-1.9019109   0.8097745   2.3655713 ]\n",
      " [ 0.21324721  0.01012013  0.02096406]\n",
      " [ 1.1092927   1.0762589   0.245802  ]]\n",
      "97 0.6407471 [[-1.9067186   0.80979836  2.3703551 ]\n",
      " [ 0.21355315  0.01063891  0.02013935]\n",
      " [ 1.1108838   1.0759459   0.24452387]]\n",
      "98 0.63977563 [[-1.9115113   0.80982697  2.3751192 ]\n",
      " [ 0.2138617   0.01114609  0.01932361]\n",
      " [ 1.1124762   1.0756313   0.24324605]]\n",
      "99 0.6388113 [[-1.9162908   0.8098621   2.3798635 ]\n",
      " [ 0.2141633   0.01165179  0.01851632]\n",
      " [ 1.1140606   1.0753249   0.24196808]]\n",
      "100 0.6378533 [[-1.9210556   0.809902    2.3845882 ]\n",
      " [ 0.21446663  0.01214702  0.01771776]\n",
      " [ 1.1156456   1.0750175   0.24069051]]\n",
      "101 0.63690233 [[-1.9258071   0.8099482   2.3892937 ]\n",
      " [ 0.21476367  0.0126403   0.01692744]\n",
      " [ 1.1172235   1.0747173   0.2394129 ]]\n",
      "102 0.63595784 [[-1.9305444   0.80999917  2.39398   ]\n",
      " [ 0.2150618   0.01312403  0.01614558]\n",
      " [ 1.1188015   1.0744164   0.23813574]]\n",
      "103 0.63502 [[-1.9352686   0.8100561   2.3986473 ]\n",
      " [ 0.21535417  0.01360551  0.01537172]\n",
      " [ 1.120373    1.074122    0.23685862]]\n",
      "104 0.6340884 [[-1.939979    0.8101178   2.403296  ]\n",
      " [ 0.21564703  0.01407831  0.01460604]\n",
      " [ 1.1219443   1.0738273   0.23558195]]\n",
      "105 0.6331631 [[-1.9446764   0.81018525  2.4079258 ]\n",
      " [ 0.21593463  0.01454855  0.01384819]\n",
      " [ 1.1235098   1.0735383   0.23430546]]\n",
      "106 0.6322441 [[-1.9493601   0.81025743  2.4125373 ]\n",
      " [ 0.21622224  0.01501083  0.0130983 ]\n",
      " [ 1.1250747   1.0732493   0.23302948]]\n",
      "107 0.63133126 [[-1.9540312   0.8103352   2.4171307 ]\n",
      " [ 0.21650492  0.01547041  0.01235607]\n",
      " [ 1.1266344   1.0729654   0.23175381]]\n",
      "108 0.6304244 [[-1.958689    0.81041765  2.421706  ]\n",
      " [ 0.21678717  0.01592267  0.01162155]\n",
      " [ 1.1281931   1.0726818   0.23047869]]\n",
      "109 0.62952363 [[-1.9633341   0.81050545  2.4262633 ]\n",
      " [ 0.21706499  0.01637197  0.01089444]\n",
      " [ 1.1297472   1.0724025   0.2292039 ]]\n",
      "110 0.62862885 [[-1.9679662   0.81059784  2.430803  ]\n",
      " [ 0.21734191  0.01681462  0.01017485]\n",
      " [ 1.1313      1.0721238   0.22792971]]\n",
      "111 0.6277399 [[-1.9725859   0.81069547  2.4353251 ]\n",
      " [ 0.21761474  0.01725418  0.00946249]\n",
      " [ 1.1328485   1.071849    0.22665596]]\n",
      "112 0.62685657 [[-1.9771928   0.81079763  2.4398298 ]\n",
      " [ 0.21788643  0.01768754  0.00875745]\n",
      " [ 1.1343958   1.0715748   0.22538288]]\n",
      "113 0.6259789 [[-1.9817874   0.8109048   2.4443173 ]\n",
      " [ 0.21815412  0.01811784  0.00805946]\n",
      " [ 1.135939    1.0713042   0.2241103 ]]\n",
      "114 0.62510693 [[-1.9863695   0.81101644  2.4487877 ]\n",
      " [ 0.21842057  0.01854229  0.00736855]\n",
      " [ 1.1374809   1.0710342   0.22283842]]\n",
      "115 0.6242404 [[-1.9909395   0.811133    2.453241  ]\n",
      " [ 0.21868312  0.01896374  0.00668453]\n",
      " [ 1.1390189   1.0707674   0.22156711]]\n",
      "116 0.6233796 [[-1.9954972   0.811254    2.4576778 ]\n",
      " [ 0.21894428  0.01937974  0.0060074 ]\n",
      " [ 1.1405555   1.0705013   0.22029653]]\n",
      "117 0.62252384 [[-2.0000432   0.81137973  2.462098  ]\n",
      " [ 0.21920179  0.01979266  0.00533696]\n",
      " [ 1.1420885   1.0702382   0.21902658]]\n",
      "118 0.6216737 [[-2.004577    0.8115097   2.4665015 ]\n",
      " [ 0.21945776  0.02020039  0.00467325]\n",
      " [ 1.1436201   1.0699757   0.21775745]]\n",
      "119 0.62082875 [[-2.009099    0.81164443  2.4708889 ]\n",
      " [ 0.21971013  0.02060519  0.00401609]\n",
      " [ 1.1451483   1.069716    0.21648902]]\n",
      "120 0.6199891 [[-2.0136092   0.8117834   2.47526   ]\n",
      " [ 0.21996084  0.02100514  0.00336543]\n",
      " [ 1.146675    1.0694569   0.21522139]]\n",
      "121 0.61915445 [[-2.018108    0.8119268   2.4796152 ]\n",
      " [ 0.22020817  0.0214021   0.00272115]\n",
      " [ 1.1481985   1.0692003   0.21395452]]\n",
      "122 0.6183249 [[-2.0225949e+00  8.1207436e-01  2.4839547e+00]\n",
      " [ 2.2045366e-01  2.1794541e-02  2.0832242e-03]\n",
      " [ 1.1497203e+00  1.0689445e+00  2.1268852e-01]]\n",
      "123 0.6175003 [[-2.0270705e+00  8.1222630e-01  2.4882784e+00]\n",
      " [ 2.2069603e-01  2.2183899e-02  1.4514982e-03]\n",
      " [ 1.1512393e+00  1.0686907e+00  2.1142332e-01]]\n",
      "124 0.61668074 [[-2.0315347e+00  8.1238234e-01  2.4925866e+00]\n",
      " [ 2.2093633e-01  2.2569168e-02  8.2594750e-04]\n",
      " [ 1.1527565e+00  1.0684378e+00  2.1015902e-01]]\n",
      "125 0.6158663 [[-2.0359876e+00  8.1254268e-01  2.4968793e+00]\n",
      " [ 2.2117367e-01  2.2951314e-02  2.0644342e-04]\n",
      " [ 1.1542710e+00  1.0681868e+00  2.0889556e-01]]\n",
      "126 0.6150564 [[-2.0404294e+00  8.1270701e-01  2.5011568e+00]\n",
      " [ 2.2140886e-01  2.3329604e-02 -4.0703162e-04]\n",
      " [ 1.1557837e+00  1.0679365e+00  2.0763305e-01]]\n",
      "127 0.61425126 [[-2.0448601e+00  8.1287551e-01  2.5054190e+00]\n",
      " [ 2.2164118e-01  2.3704866e-02 -1.0146152e-03]\n",
      " [ 1.1572938e+00  1.0676880e+00  2.0637143e-01]]\n",
      "128 0.6134509 [[-2.0492799e+00  8.1304795e-01  2.5096662e+00]\n",
      " [ 2.2187132e-01  2.4076467e-02 -1.6163401e-03]\n",
      " [ 1.1588020e+00  1.0674404e+00  2.0511077e-01]]\n",
      "129 0.61265516 [[-2.0536888e+00  8.1322443e-01  2.5138986e+00]\n",
      " [ 2.2209877e-01  2.4444992e-02 -2.2123181e-03]\n",
      " [ 1.1603080e+00  1.0671941e+00  2.0385104e-01]]\n",
      "130 0.6118642 [[-2.0580869   0.8134048   2.5181162 ]\n",
      " [ 0.22232383  0.02481018 -0.00280258]\n",
      " [ 1.161812    1.0669489   0.20259233]]\n",
      "131 0.61107755 [[-2.0624743   0.8135891   2.5223193 ]\n",
      " [ 0.22254637  0.02517231 -0.00338723]\n",
      " [ 1.1633137   1.0667049   0.2013346 ]]\n",
      "132 0.6102957 [[-2.066851    0.8137772   2.5265079 ]\n",
      " [ 0.22276653  0.02553124 -0.00396633]\n",
      " [ 1.1648135   1.0664618   0.20007788]]\n",
      "133 0.60951805 [[-2.071217    0.81396914  2.530682  ]\n",
      " [ 0.22298416  0.02588723 -0.00453995]\n",
      " [ 1.1663111   1.0662198   0.19882217]]\n",
      "134 0.60874486 [[-2.0755727   0.8141648   2.534842  ]\n",
      " [ 0.22319949  0.02624014 -0.00510817]\n",
      " [ 1.1678069   1.0659788   0.19756752]]\n",
      "135 0.60797596 [[-2.079918    0.81436425  2.5389879 ]\n",
      " [ 0.22341232  0.02659016 -0.00567103]\n",
      " [ 1.1693006   1.0657387   0.19631395]]\n",
      "136 0.60721135 [[-2.0842528   0.8145673   2.5431197 ]\n",
      " [ 0.22362272  0.02693734 -0.00622862]\n",
      " [ 1.1707922   1.0654995   0.19506143]]\n",
      "137 0.60645115 [[-2.0885775   0.81477404  2.5472376 ]\n",
      " [ 0.22383085  0.02728159 -0.00678101]\n",
      " [ 1.172282    1.0652612   0.19381   ]]\n",
      "138 0.6056951 [[-2.092892    0.8149844   2.5513418 ]\n",
      " [ 0.22403654  0.02762314 -0.00732824]\n",
      " [ 1.1737697   1.0650238   0.1925597 ]]\n",
      "139 0.60494316 [[-2.0971963   0.81519824  2.5554323 ]\n",
      " [ 0.22423993  0.02796191 -0.0078704 ]\n",
      " [ 1.1752555   1.0647871   0.19131051]]\n",
      "140 0.6041954 [[-2.1014907   0.8154157   2.5595093 ]\n",
      " [ 0.22444095  0.02829804 -0.00840755]\n",
      " [ 1.1767395   1.0645514   0.19006243]]\n",
      "141 0.6034515 [[-2.105775    0.8156366   2.563573  ]\n",
      " [ 0.22463968  0.02863148 -0.00893973]\n",
      " [ 1.1782215   1.0643163   0.18881552]]\n",
      "142 0.6027117 [[-2.1100497   0.8158609   2.5676231 ]\n",
      " [ 0.22483611  0.02896233 -0.00946702]\n",
      " [ 1.1797016   1.0640819   0.18756975]]\n",
      "143 0.6019759 [[-2.1143146   0.8160887   2.5716603 ]\n",
      " [ 0.22503027  0.02929064 -0.00998947]\n",
      " [ 1.1811799   1.0638483   0.18632516]]\n",
      "144 0.60124403 [[-2.1185696   0.8163198   2.5756843 ]\n",
      " [ 0.22522213  0.02961646 -0.01050714]\n",
      " [ 1.1826563   1.0636153   0.18508175]]\n",
      "145 0.6005161 [[-2.1228151   0.8165543   2.5796955 ]\n",
      " [ 0.22541174  0.02993981 -0.01102011]\n",
      " [ 1.1841308   1.063383    0.18383951]]\n",
      "146 0.59979177 [[-2.127051    0.8167921   2.5836937 ]\n",
      " [ 0.22559917  0.0302607  -0.01152843]\n",
      " [ 1.1856035   1.0631512   0.18259847]]\n",
      "147 0.5990715 [[-2.1312776   0.8170331   2.5876791 ]\n",
      " [ 0.22578438  0.03057921 -0.01203215]\n",
      " [ 1.1870744   1.0629201   0.18135864]]\n",
      "148 0.5983549 [[-2.1354947   0.8172774   2.591652  ]\n",
      " [ 0.22596735  0.03089541 -0.01253132]\n",
      " [ 1.1885436   1.0626897   0.18012002]]\n",
      "149 0.59764194 [[-2.1397026   0.81752485  2.5956123 ]\n",
      " [ 0.22614822  0.03120921 -0.01302599]\n",
      " [ 1.1900109   1.0624597   0.17888266]]\n",
      "150 0.5969327 [[-2.143901    0.8177755   2.5995603 ]\n",
      " [ 0.22632685  0.03152078 -0.01351622]\n",
      " [ 1.1914765   1.0622303   0.17764653]]\n",
      "151 0.5962272 [[-2.1480904   0.8180292   2.6034958 ]\n",
      " [ 0.22650339  0.03183014 -0.01400209]\n",
      " [ 1.1929402   1.0620015   0.17641163]]\n",
      "152 0.59552515 [[-2.1522706   0.81828606  2.6074193 ]\n",
      " [ 0.22667779  0.03213724 -0.01448362]\n",
      " [ 1.1944022   1.0617731   0.17517799]]\n",
      "153 0.5948267 [[-2.1564417   0.818546    2.6113305 ]\n",
      " [ 0.2268501   0.03244217 -0.01496089]\n",
      " [ 1.1958625   1.0615451   0.1739456 ]]\n",
      "154 0.59413177 [[-2.1606038   0.818809    2.6152296 ]\n",
      " [ 0.22702028  0.03274502 -0.01543391]\n",
      " [ 1.197321    1.0613178   0.1727145 ]]\n",
      "155 0.5934402 [[-2.164757    0.8190749   2.6191168 ]\n",
      " [ 0.22718845  0.03304567 -0.01590275]\n",
      " [ 1.1987779   1.0610908   0.17148471]]\n",
      "156 0.59275234 [[-2.1689012   0.8193438   2.6229923 ]\n",
      " [ 0.22735459  0.03334426 -0.01636746]\n",
      " [ 1.200233    1.0608642   0.17025618]]\n",
      "157 0.5920677 [[-2.1730368   0.81961566  2.6268559 ]\n",
      " [ 0.22751863  0.03364086 -0.01682811]\n",
      " [ 1.2016864   1.0606381   0.16902894]]\n",
      "158 0.5913867 [[-2.1771636   0.81989044  2.6307077 ]\n",
      " [ 0.22768068  0.03393545 -0.01728474]\n",
      " [ 1.203138    1.0604124   0.16780299]]\n",
      "159 0.5907087 [[-2.1812816   0.820168    2.6345482 ]\n",
      " [ 0.22784083  0.03422792 -0.01773736]\n",
      " [ 1.204588    1.060187    0.16657837]]\n",
      "160 0.5900341 [[-2.185391    0.8204485   2.6383772 ]\n",
      " [ 0.22799881  0.03451864 -0.01818605]\n",
      " [ 1.2060362   1.0599622   0.16535506]]\n",
      "161 0.5893628 [[-2.1894917   0.82073176  2.6421947 ]\n",
      " [ 0.22815503  0.03480721 -0.01863085]\n",
      " [ 1.2074829   1.0597374   0.16413307]]\n",
      "162 0.5886949 [[-2.1935842   0.82101786  2.646001  ]\n",
      " [ 0.22830912  0.03509409 -0.01907182]\n",
      " [ 1.2089278   1.0595133   0.16291238]]\n",
      "163 0.58803 [[-2.197668    0.82130665  2.6497962 ]\n",
      " [ 0.22846155  0.03537881 -0.01950896]\n",
      " [ 1.2103711   1.0592892   0.16169307]]\n",
      "164 0.58736825 [[-2.2017436   0.82159823  2.6535802 ]\n",
      " [ 0.22861177  0.035662   -0.01994237]\n",
      " [ 1.2118126   1.0590658   0.16047506]]\n",
      "165 0.5867097 [[-2.2058108   0.82189244  2.6573532 ]\n",
      " [ 0.2287604   0.03594304 -0.02037205]\n",
      " [ 1.2132527   1.0588424   0.15925843]]\n",
      "166 0.58605427 [[-2.2098699   0.82218933  2.6611152 ]\n",
      " [ 0.22890696  0.03622252 -0.02079806]\n",
      " [ 1.2146909   1.0586195   0.15804313]]\n",
      "167 0.585402 [[-2.2139206   0.82248884  2.6648664 ]\n",
      " [ 0.2290518   0.03650007 -0.02122045]\n",
      " [ 1.2161276   1.0583967   0.15682918]]\n",
      "168 0.5847526 [[-2.2179632   0.82279104  2.668607  ]\n",
      " [ 0.22919469  0.036776   -0.02163927]\n",
      " [ 1.2175627   1.0581744   0.15561655]]\n",
      "169 0.5841065 [[-2.2219977   0.82309574  2.6723368 ]\n",
      " [ 0.2293359   0.03705004 -0.02205453]\n",
      " [ 1.2189962   1.057952    0.15440533]]\n",
      "170 0.5834632 [[-2.2260244   0.82340306  2.6760561 ]\n",
      " [ 0.22947514  0.03732257 -0.02246629]\n",
      " [ 1.2204279   1.0577302   0.15319544]]\n",
      "171 0.5828229 [[-2.230043    0.8237129   2.6797647 ]\n",
      " [ 0.22961277  0.03759326 -0.02287461]\n",
      " [ 1.2218581   1.0575085   0.15198691]]\n",
      "172 0.5821854 [[-2.2340536   0.8240252   2.683463  ]\n",
      " [ 0.22974852  0.03786239 -0.02327949]\n",
      " [ 1.2232866   1.0572871   0.15077977]]\n",
      "173 0.58155084 [[-2.2380564   0.82434005  2.687151  ]\n",
      " [ 0.22988266  0.03812974 -0.02368097]\n",
      " [ 1.2247137   1.0570658   0.14957401]]\n",
      "174 0.58091927 [[-2.2420514   0.8246573   2.6908286 ]\n",
      " [ 0.23001498  0.03839556 -0.02407911]\n",
      " [ 1.2261391   1.0568448   0.14836963]]\n",
      "175 0.5802907 [[-2.2460384   0.82497704  2.694496  ]\n",
      " [ 0.23014565  0.03865974 -0.02447395]\n",
      " [ 1.2275629   1.056624    0.14716661]]\n",
      "176 0.57966465 [[-2.250018    0.82529914  2.6981533 ]\n",
      " [ 0.23027454  0.03892238 -0.0248655 ]\n",
      " [ 1.2289851   1.0564035   0.145965  ]]\n",
      "177 0.57904136 [[-2.2539897   0.82562363  2.7018006 ]\n",
      " [ 0.23040186  0.03918339 -0.02525383]\n",
      " [ 1.2304058   1.0561831   0.14476477]]\n",
      "178 0.5784211 [[-2.257954    0.8259505   2.705438  ]\n",
      " [ 0.2305274   0.03944298 -0.02563894]\n",
      " [ 1.2318248   1.0559629   0.14356592]]\n",
      "179 0.5778034 [[-2.2619104   0.82627964  2.7090652 ]\n",
      " [ 0.23065142  0.03970092 -0.02602089]\n",
      " [ 1.2332423   1.0557429   0.14236848]]\n",
      "180 0.5771885 [[-2.2658596   0.8266111   2.7126827 ]\n",
      " [ 0.23077375  0.03995743 -0.02639973]\n",
      " [ 1.2346581   1.055523    0.14117241]]\n",
      "181 0.57657623 [[-2.2698011   0.8269448   2.7162905 ]\n",
      " [ 0.23089454  0.04021238 -0.02677548]\n",
      " [ 1.2360725   1.0553033   0.13997774]]\n",
      "182 0.57596666 [[-2.2737353   0.8272808   2.7198887 ]\n",
      " [ 0.23101367  0.04046591 -0.02714814]\n",
      " [ 1.2374853   1.0550838   0.1387845 ]]\n",
      "183 0.5753597 [[-2.277662    0.8276191   2.7234771 ]\n",
      " [ 0.23113118  0.04071807 -0.0275178 ]\n",
      " [ 1.2388965   1.0548644   0.13759263]]\n",
      "184 0.5747554 [[-2.2815814   0.82795954  2.727056  ]\n",
      " [ 0.23124728  0.04096862 -0.02788444]\n",
      " [ 1.2403063   1.0546451   0.13640219]]\n",
      "185 0.57415366 [[-2.2854936   0.8283022   2.7306256 ]\n",
      " [ 0.23136166  0.04121795 -0.02824814]\n",
      " [ 1.2417142   1.0544261   0.13521314]]\n",
      "186 0.5735546 [[-2.2893984   0.828647    2.7341857 ]\n",
      " [ 0.23147477  0.04146561 -0.02860892]\n",
      " [ 1.243121    1.054207    0.13402548]]\n",
      "187 0.572958 [[-2.2932963   0.82899404  2.7377365 ]\n",
      " [ 0.23158602  0.04171226 -0.02896679]\n",
      " [ 1.2445259   1.0539883   0.13283925]]\n",
      "188 0.5723637 [[-2.2971869   0.8293431   2.741278  ]\n",
      " [ 0.23169611  0.04195717 -0.02932179]\n",
      " [ 1.2459296   1.0537695   0.13165443]]\n",
      "189 0.5717722 [[-2.3010705   0.8296943   2.7448103 ]\n",
      " [ 0.23180445  0.04220103 -0.02967397]\n",
      " [ 1.2473315   1.0535511   0.13047099]]\n",
      "190 0.57118297 [[-2.304947    0.8300475   2.7483337 ]\n",
      " [ 0.23191161  0.04244322 -0.03002333]\n",
      " [ 1.2487321   1.0533324   0.129289  ]]\n",
      "191 0.5705962 [[-2.3088164   0.83040285  2.7518477 ]\n",
      " [ 0.23201701  0.04268444 -0.03036995]\n",
      " [ 1.2501309   1.0531143   0.12810837]]\n",
      "192 0.570012 [[-2.312679    0.8307602   2.755353  ]\n",
      " [ 0.23212126  0.04292402 -0.03071379]\n",
      " [ 1.2515284   1.0528959   0.1269292 ]]\n",
      "193 0.5694301 [[-2.3165348   0.83111954  2.7588491 ]\n",
      " [ 0.2322239   0.04316253 -0.03105495]\n",
      " [ 1.2529242   1.0526779   0.12575139]]\n",
      "194 0.56885064 [[-2.3203835   0.83148086  2.7623365 ]\n",
      " [ 0.23232532  0.04339959 -0.03139342]\n",
      " [ 1.2543186   1.0524598   0.12457503]]\n",
      "195 0.56827366 [[-2.3242254   0.8318442   2.765815  ]\n",
      " [ 0.23242523  0.0436355  -0.03172924]\n",
      " [ 1.2557114   1.0522419   0.12340005]]\n",
      "196 0.567699 [[-2.3280606   0.83220947  2.7692847 ]\n",
      " [ 0.23252383  0.04387008 -0.03206243]\n",
      " [ 1.2571028   1.0520241   0.12222651]]\n",
      "197 0.56712675 [[-2.331889    0.8325767   2.7727458 ]\n",
      " [ 0.232621    0.04410348 -0.032393  ]\n",
      " [ 1.2584927   1.0518064   0.12105439]]\n",
      "198 0.56655645 [[-2.3357105   0.8329458   2.7761984 ]\n",
      " [ 0.23271689  0.0443356  -0.032721  ]\n",
      " [ 1.259881    1.0515888   0.11988369]]\n",
      "199 0.56598866 [[-2.3395255   0.8333168   2.7796423 ]\n",
      " [ 0.23281145  0.04456649 -0.03304647]\n",
      " [ 1.2612679   1.0513712   0.11871439]]\n",
      "200 0.56542313 [[-2.3433337   0.8336897   2.7830777 ]\n",
      " [ 0.23290464  0.04479625 -0.03336943]\n",
      " [ 1.2626532   1.0511538   0.11754649]]\n",
      "Prediction: [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "X = tf.placeholder(\"float\", [None, 3]) # placeholder를 통해 training, test나눔\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "# learning rate 설정 -> 너무 크면 발산, 너무 작으면 local dilemma\n",
    "\n",
    "prediction = tf.arg_max(hypothesis, 1) # 예측\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "   sess.run(tf.global_variables_initializer())\n",
    "   for step in range(201):\n",
    "       cost_val, W_val, _ = sess.run([cost, W, optimizer], \n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "       print(step, cost_val, W_val)\n",
    "    # test set사용해서 평가\n",
    "   print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "   print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaN - non-normailized inputs 했을때 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  3286798700000.0 \n",
      "Prediction:\n",
      " [[-1278151. ]\n",
      " [-2573456.2]\n",
      " [-2024362. ]\n",
      " [-1418951. ]\n",
      " [-1672377.9]\n",
      " [-1686459.4]\n",
      " [-1545668.1]\n",
      " [-1968053.6]]\n",
      "1 Cost:  3.611142e+27 \n",
      "Prediction:\n",
      " [[4.2389041e+13]\n",
      " [8.5333484e+13]\n",
      " [6.7128774e+13]\n",
      " [4.7056912e+13]\n",
      " [5.5459088e+13]\n",
      " [5.5925876e+13]\n",
      " [5.1258002e+13]\n",
      " [6.5261621e+13]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[-1.4050420e+21]\n",
      " [-2.8284936e+21]\n",
      " [-2.2250739e+21]\n",
      " [-1.5597650e+21]\n",
      " [-1.8382663e+21]\n",
      " [-1.8537387e+21]\n",
      " [-1.6990158e+21]\n",
      " [-2.1631846e+21]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[4.6572016e+28]\n",
      " [9.3754247e+28]\n",
      " [7.3753079e+28]\n",
      " [5.1700515e+28]\n",
      " [6.0931821e+28]\n",
      " [6.1444670e+28]\n",
      " [5.6316166e+28]\n",
      " [7.1701674e+28]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[-1.5436925e+36]\n",
      " [-3.1076115e+36]\n",
      " [-2.4446455e+36]\n",
      " [-1.7136834e+36]\n",
      " [-2.0196677e+36]\n",
      " [-2.0366667e+36]\n",
      " [-1.8666756e+36]\n",
      " [-2.3766490e+36]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# nomailize x\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "# nomalize 했을때  \n",
    "xy = MinMaxScaler(xy)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 3.144801644\n",
      "Epoch: 0002 cost = 1.122395613\n",
      "Epoch: 0003 cost = 0.878235579\n",
      "Epoch: 0004 cost = 0.761424967\n",
      "Epoch: 0005 cost = 0.690545581\n",
      "Epoch: 0006 cost = 0.641129860\n",
      "Epoch: 0007 cost = 0.603962244\n",
      "Epoch: 0008 cost = 0.575177942\n",
      "Epoch: 0009 cost = 0.551365742\n",
      "Epoch: 0010 cost = 0.531249742\n",
      "Epoch: 0011 cost = 0.514629601\n",
      "Epoch: 0012 cost = 0.499557519\n",
      "Epoch: 0013 cost = 0.486708446\n",
      "Epoch: 0014 cost = 0.475541684\n",
      "Epoch: 0015 cost = 0.464533331\n",
      "Learning finished\n",
      "Accuracy:  0.8892\n",
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f68080f5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes])) # shape\n",
    "b = tf.Variable(tf.random_normal([nb_classes])) # 10개\n",
    "\n",
    "# softmax 사용\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "#test mddel\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "# calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "training_epochs = 15 # epoch 전체 데이터 set을 한번 학습시키는 것이 1epoch\n",
    "batch_size = 100 # 한번에 100개씩 학습\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap='Greys',\n",
    "        interpolation='nearest')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
